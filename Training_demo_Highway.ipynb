{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f64f4e",
   "metadata": {},
   "source": [
    "# Training demo: Highway\n",
    "\n",
    "## Installation\n",
    "\n",
    "Set up Stablebaseline3 environment:\n",
    "\n",
    "```\n",
    "\n",
    "# using pip\n",
    "pip install swig cmake ffmpeg\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# using Conda\n",
    "conda create --name <env_name> python=3.9\n",
    "conda activate <env_name>\n",
    "pip install swig cmake ffmpeg\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ba5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import DQN, DQN_ME, ResidualSoftDQN\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3e1f5",
   "metadata": {},
   "source": [
    "## Define the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb2d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    episode_lane = 0.0\n",
    "    episode_speed = 0.0\n",
    "    episode_reward = 0.0\n",
    "    basic_reward = 0.0\n",
    "    added_reward = 0.0\n",
    "    episode_added_rewards,episode_basic_rewards = [],[]\n",
    "    episode_rewards, episode_lengths, episode_speeds, episode_lanes = [], [], [], []\n",
    "    ep_len = 0\n",
    "    successes = []\n",
    "    env = gym.make(\"highway-ME-basic-AddRightRewardALL-v0\")\n",
    "    obs = env.reset()\n",
    "    for _ in range(400):\n",
    "            action, lstm_states = model.predict(\n",
    "                obs,  \n",
    "                deterministic = True,\n",
    "            )\n",
    "            obs, reward, done, infos = env.step(int (action))\n",
    "            \n",
    "            neighbours = env.road.network.all_side_lanes(env.vehicle.lane_index)\n",
    "            lane = env.vehicle.target_lane_index[2] if isinstance(env.vehicle, ControlledVehicle) \\\n",
    "                else env.vehicle.lane_index[2]\n",
    "            lane = lane / max(len(neighbours) - 1, 1)\n",
    "            \n",
    "            episode_lane = episode_lane + lane\n",
    "            \n",
    "            forward_speed = env.vehicle.speed * np.cos(env.vehicle.heading)\n",
    "            \n",
    "            episode_speed = episode_speed + forward_speed\n",
    "            \n",
    "            basic_reward +=  env.basic_reward\n",
    "            added_reward +=  env.added_reward\n",
    "            episode_reward += reward\n",
    "            ep_len += 1\n",
    "            \n",
    "            if done:\n",
    "                if ep_len == 40:\n",
    "                    successes.append(1)\n",
    "                    episode_speeds.append(episode_speed/ep_len) \n",
    "                    episode_lanes.append(episode_lane/ep_len) \n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    episode_added_rewards.append(added_reward)\n",
    "                    episode_basic_rewards.append(basic_reward)\n",
    "                    episode_lengths.append(ep_len)\n",
    "\n",
    "                else:\n",
    "                    successes.append(0)\n",
    "                    episode_speeds.append(episode_speed/ep_len) \n",
    "                    episode_lanes.append(episode_lane/ep_len) \n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    episode_added_rewards.append(added_reward)\n",
    "                    episode_basic_rewards.append(basic_reward)\n",
    "                    episode_lengths.append(ep_len)\n",
    "                episode_reward = 0.0\n",
    "                added_reward = 0.0\n",
    "                basic_reward = 0.0\n",
    "                ep_len = 0\n",
    "                episode_speed = 0.0\n",
    "                episode_lane = 0.0\n",
    "                obs = env.reset()\n",
    "                \n",
    "    print(f\"Success rate: {100 * np.mean(successes):.2f}%\")\n",
    "    print(f\"{len(successes)} Episodes\")\n",
    "    print(f\"Mean reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Mean basic reward: {np.mean(episode_basic_rewards):.2f} +/- {np.std(episode_basic_rewards):.2f}\")\n",
    "    print(f\"Mean added reward: {np.mean(episode_added_rewards):.2f} +/- {np.std(episode_added_rewards):.2f}\")\n",
    "    print(f\"Mean episode length: {np.mean(episode_lengths):.2f} +/- {np.std(episode_lengths):.2f}\")\n",
    "    print(f\"Mean lane: {np.mean(episode_lanes):.2f} +/- {np.std(episode_lanes):.2f}\")\n",
    "    print(f\"Mean speed: {np.mean(episode_speeds):.2f} +/- {np.std(episode_speeds):.2f}\")\n",
    "    print(\"________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f9623",
   "metadata": {},
   "source": [
    "## Train RL priror policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198cd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init env\n",
    "env = gym.make(\"highway-ME-basic-v0\")\n",
    "\n",
    "# Init soft Q model\n",
    "model = DQN_ME(\n",
    "    env=env,\n",
    "    policy='MlpPolicy',\n",
    "    batch_size=32,\n",
    "    buffer_size=15000,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.8,\n",
    "    target_update_interval=50,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    exploration_fraction=0.7,\n",
    "    policy_kwargs=dict(net_arch=[256, 256])\n",
    ")\n",
    "\n",
    "# Training. For better performance, we recomand train 5e5 steps \n",
    "model.learn(int(1e5))\n",
    "\n",
    "# Save model\n",
    "model.save(f\"./logs/highway-ME-basic-v0_Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e9616",
   "metadata": {},
   "source": [
    "## Train customized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed4d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init env\n",
    "env = gym.make(\"highway-ME-basic-AddRightReward-v0\")\n",
    "\n",
    "# Init residual soft Q model\n",
    "model_cust = ResidualSoftDQN(\n",
    "    env=env,\n",
    "    prior_model_path='./logs/highway-ME-basic-v0_Example_Pretrained',\n",
    "    policy='MlpPolicy',\n",
    "    batch_size=32,\n",
    "    buffer_size=15000,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.8,\n",
    "    target_update_interval=50,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    exploration_fraction=0.7,\n",
    "    policy_kwargs=dict(net_arch=[256, 256])\n",
    ")\n",
    "\n",
    "# Training. For better performance, we recomand train 5e5 steps \n",
    "model_cust.learn(int(1e5))\n",
    "\n",
    "# Save model \n",
    "model_cust.save(f\"./logs/highway-ME-basic-AddRightReward-v0_Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(model_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5b22c",
   "metadata": {},
   "source": [
    "## Residual MCTS\n",
    "\n",
    "\n",
    "## Define the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "from stable_baselines3 import DQN, DQN_ME, ResidualSoftDQN\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from itertools import product\n",
    "from multiprocessing.pool import Pool\n",
    "from rl_agents.agents.common.factory import load_agent, load_environment\n",
    "from highway_env.vehicle.controller import ControlledVehicle\n",
    "from highway_env import utils\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluation(agent, env, evalsteps):\n",
    "    episode_lane = 0.0\n",
    "    episode_speed = 0.0\n",
    "    episode_reward = 0.0\n",
    "    total_reward = 0.0\n",
    "    episode_rewards, episode_lengths, episode_speeds, episode_lanes = [], [], [], []\n",
    "    episode_total_rewards = []\n",
    "    ep_len = 0\n",
    "    successes = []\n",
    "    obs = env.reset()\n",
    "    for timestep in range(evalsteps):\n",
    "        actions = agent.plan(obs)\n",
    "        action = actions[0]\n",
    "        obs, reward, done, infos = env.step(int (action))\n",
    "        print(\"time : \",timestep, \"; action:\",  action)\n",
    "        neighbours = env.road.network.all_side_lanes(env.vehicle.lane_index)\n",
    "        lane = env.vehicle.target_lane_index[2] if isinstance(env.vehicle, ControlledVehicle) \\\n",
    "            else env.vehicle.lane_index[2]\n",
    "        lane = lane / max(len(neighbours) - 1, 1)\n",
    "\n",
    "        episode_lane = episode_lane + lane\n",
    "\n",
    "        forward_speed = env.vehicle.speed * np.cos(env.vehicle.heading)\n",
    "\n",
    "        episode_speed = episode_speed + forward_speed\n",
    "\n",
    "        episode_reward += reward\n",
    "        ep_len += 1\n",
    "\n",
    "        \n",
    "        scaled_speed = utils.lmap(forward_speed, env.config[\"reward_speed_range\"], [0, 1])\n",
    "        \n",
    "        reward = \\\n",
    "            + (-0.5) * env.vehicle.crashed \\\n",
    "            + 0.0 * lane \\\n",
    "            + 0.4 * np.clip(scaled_speed, 0, 1)  + 1\n",
    "        reward = 0 if not env.vehicle.on_road else reward\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            if ep_len == 40:\n",
    "                successes.append(1)\n",
    "                episode_speeds.append(episode_speed/ep_len) \n",
    "                episode_lanes.append(episode_lane/ep_len) \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_total_rewards.append(total_reward)\n",
    "                episode_lengths.append(ep_len)\n",
    "\n",
    "            else:\n",
    "                successes.append(0)\n",
    "                episode_speeds.append(episode_speed/ep_len) \n",
    "                episode_lanes.append(episode_lane/ep_len) \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_total_rewards.append(total_reward)\n",
    "                episode_lengths.append(ep_len)\n",
    "            episode_reward = 0.0\n",
    "            total_reward = 0.0\n",
    "            ep_len = 0\n",
    "            episode_speed = 0.0\n",
    "            episode_lane = 0.0\n",
    "            obs = env.reset()\n",
    "            print(f\"Success rate: {100 * np.mean(successes):.2f}%\")\n",
    "            print(f\"{len(successes)} Episodes\")\n",
    "            print(f\"Mean added reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "            print(f\"Mean basic reward: {np.mean(episode_total_rewards):.2f} +/- {np.std(episode_total_rewards):.2f}\")\n",
    "            print(f\"Mean episode length: {np.mean(episode_lengths):.2f} +/- {np.std(episode_lengths):.2f}\")\n",
    "            print(f\"Mean lane: {np.mean(episode_lanes):.2f} +/- {np.std(episode_lanes):.2f}\")\n",
    "            print(f\"Mean speed: {np.mean(episode_speeds):.2f} +/- {np.std(episode_speeds):.2f}\")\n",
    "            print(\"-----------\")\n",
    "            \n",
    "    print(\"________________________________________________\")\n",
    "    print(f\"Success rate: {100 * np.mean(successes):.2f}%\")\n",
    "    print(f\"{len(successes)} Episodes\")\n",
    "    print(f\"Mean reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Mean episode length: {np.mean(episode_lengths):.2f} +/- {np.std(episode_lengths):.2f}\")\n",
    "    print(f\"Mean lane: {np.mean(episode_lanes):.2f} +/- {np.std(episode_lanes):.2f}\")\n",
    "    print(f\"Mean speed: {np.mean(episode_speeds):.2f} +/- {np.std(episode_speeds):.2f}\")\n",
    "    print(\"________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be5f7b",
   "metadata": {},
   "source": [
    "## Running online MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init env\n",
    "env = gym.make(\"highway-ME-basic-AddRightReward-v0\")\n",
    "obs = env.reset()\n",
    "\n",
    "# Load hyperparams. If you want to change the prior model, you can change the following config file.\n",
    "agent_config = \"./configs/HighwayEnv/agents/MCTSAgent/baselineMEPreRL.json\"\n",
    "agent = load_agent(agent_config, env)\n",
    "\n",
    "# Simulation\n",
    "evalsteps = 40\n",
    "evaluation(agent,env, evalsteps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('resQ-submit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2d81ac56973ba53ea0955d2b6751274477c9623db60c27d456fdaa12f9abea1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
